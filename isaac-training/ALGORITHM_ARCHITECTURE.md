# NavRL ç®—æ³•æ¶æ„æ–‡æ¡£

## æ¦‚è¿°

NavRLæ˜¯ä¸€ä¸ªåŸºäº**PPO (Proximal Policy Optimization)** å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ— äººæœºå¯¼èˆªç³»ç»Ÿï¼Œä½¿ç”¨NVIDIA Isaac Simè¿›è¡Œç‰©ç†ä»¿çœŸï¼Œæ”¯æŒåŠ¨æ€éšœç¢ç‰©ç¯å¢ƒä¸‹çš„å¤šæœºå™¨äººå¯¼èˆªè®­ç»ƒã€‚

---

## 1. æ•´ä½“æ¶æ„

### 1.1 ç³»ç»Ÿç»„ä»¶å±‚æ¬¡

```
è®­ç»ƒç³»ç»Ÿ
â”œâ”€â”€ Isaac Simulation (ç‰©ç†ä»¿çœŸ)
â”‚   â”œâ”€â”€ Hummingbird æ— äººæœºæ¨¡å‹
â”‚   â”œâ”€â”€ åœ°å½¢ç”Ÿæˆå™¨ (é™æ€éšœç¢ç‰©)
â”‚   â””â”€â”€ åŠ¨æ€éšœç¢ç‰©ç³»ç»Ÿ
â”‚
â”œâ”€â”€ ç¯å¢ƒ (NavigationEnv)
â”‚   â”œâ”€â”€ è§‚æµ‹ç©ºé—´ (LiDAR + çŠ¶æ€ + åŠ¨æ€éšœç¢ç‰©ä¿¡æ¯)
â”‚   â”œâ”€â”€ åŠ¨ä½œç©ºé—´ (3Dé€Ÿåº¦æ§åˆ¶)
â”‚   â”œâ”€â”€ å¥–åŠ±å‡½æ•°
â”‚   â””â”€â”€ ä»»åŠ¡ç®¡ç† (èµ·ç‚¹/ç›®æ ‡ç‚¹ç”Ÿæˆ)
â”‚
â”œâ”€â”€ æ§åˆ¶å™¨è½¬æ¢ (LeePositionController)
â”‚   â””â”€â”€ é€Ÿåº¦æŒ‡ä»¤ â†’ å§¿æ€æ§åˆ¶æŒ‡ä»¤
â”‚
â”œâ”€â”€ PPOç®—æ³•
â”‚   â”œâ”€â”€ ç‰¹å¾æå–å™¨ (CNN + MLP)
â”‚   â”œâ”€â”€ Actorç½‘ç»œ (ç­–ç•¥ç½‘ç»œ)
â”‚   â”œâ”€â”€ Criticç½‘ç»œ (ä»·å€¼ç½‘ç»œ)
â”‚   â””â”€â”€ è®­ç»ƒå¾ªç¯ (GAE + PPO Loss)
â”‚
â””â”€â”€ æ•°æ®æ”¶é›†ä¸ç›‘æ§
    â”œâ”€â”€ SyncDataCollector (åŒæ­¥æ•°æ®é‡‡é›†)
    â””â”€â”€ Wandb (è®­ç»ƒç›‘æ§)
```

---

## 2. ç¯å¢ƒè®¾è®¡ (NavigationEnv)

### 2.1 è§‚æµ‹ç©ºé—´ (Observation Space)

ç¯å¢ƒä¸ºæ¯ä¸ªæ— äººæœºæä¾›ä»¥ä¸‹è§‚æµ‹ä¿¡æ¯ï¼š

#### **1) LiDAR ç‚¹äº‘æ•°æ®**
- **ç»´åº¦**: `(36, 4)` = æ°´å¹³36æŸ Ã— å‚ç›´4æŸ
- **å‚æ•°**:
  - æ¢æµ‹è·ç¦»: 4.0ç±³
  - æ°´å¹³åˆ†è¾¨ç‡: 10Â° (360Â°/10Â° = 36æŸ)
  - å‚ç›´è§†åœºè§’: [-10Â°, 20Â°]
  - å‚ç›´æŸæ•°: 4æŸ
- **æ•°æ®å¤„ç†**: å½’ä¸€åŒ–è·ç¦»å€¼ï¼Œæä¾›å‘¨å›´éšœç¢ç‰©çš„ç©ºé—´ä¿¡æ¯

#### **2) æ— äººæœºçŠ¶æ€ä¿¡æ¯**
åŒ…å«ä»¥ä¸‹å‘é‡ (å…·ä½“ç»´åº¦éœ€æŸ¥çœ‹`construct_input`å‡½æ•°):
- å½“å‰ä½ç½®
- ç›®æ ‡ä½ç½®/æ–¹å‘
- å½“å‰é€Ÿåº¦
- å§¿æ€ä¿¡æ¯
- ä¸ç›®æ ‡çš„ç›¸å¯¹å…³ç³»

#### **3) åŠ¨æ€éšœç¢ç‰©ä¿¡æ¯**
- **ç»´åº¦**: `(5, N)` - è¿½è¸ªæœ€è¿‘çš„5ä¸ªåŠ¨æ€éšœç¢ç‰©
- **å†…å®¹**: ç›¸å¯¹ä½ç½®ã€é€Ÿåº¦ã€å¤§å°ç­‰ä¿¡æ¯

### 2.2 åŠ¨ä½œç©ºé—´ (Action Space)

- **ç±»å‹**: è¿ç»­åŠ¨ä½œç©ºé—´
- **ç»´åº¦**: 3D (x, y, z é€Ÿåº¦åˆ†é‡)
- **èŒƒå›´**: [-2.0, 2.0] m/s (å¯é…ç½®)
- **åæ ‡ç³»è½¬æ¢**: 
  - ç­–ç•¥ç½‘ç»œè¾“å‡ºï¼šæœºä½“åæ ‡ç³»ä¸‹çš„é€Ÿåº¦
  - è‡ªåŠ¨è½¬æ¢ä¸ºï¼šä¸–ç•Œåæ ‡ç³»é€Ÿåº¦æŒ‡ä»¤
  - é€šè¿‡Lee Position Controllerè½¬æ¢ä¸ºç”µæœºæ§åˆ¶æŒ‡ä»¤

### 2.3 å¥–åŠ±å‡½æ•°è®¾è®¡

å¥–åŠ±å‡½æ•°éœ€è¦æŸ¥çœ‹`env.py`ä¸­çš„è¯¦ç»†å®ç°ï¼Œé€šå¸¸åŒ…å«ï¼š
- âœ… **ç›®æ ‡æ¥è¿‘å¥–åŠ±**: é¼“åŠ±å‘ç›®æ ‡ç§»åŠ¨
- âš ï¸ **ç¢°æ’æƒ©ç½š**: ä¸éšœç¢ç‰©ç¢°æ’çš„è´Ÿå¥–åŠ±
- ğŸ¯ **æˆåŠŸåˆ°è¾¾å¥–åŠ±**: åˆ°è¾¾ç›®æ ‡çš„å¤§é¢å¥–åŠ±
- â±ï¸ **æ—¶é—´æƒ©ç½š**: é¼“åŠ±å¿«é€Ÿå®Œæˆä»»åŠ¡
- ğŸš **å¹³æ»‘é£è¡Œå¥–åŠ±**: æƒ©ç½šè¿‡å¤§çš„é€Ÿåº¦å˜åŒ–

### 2.4 ç»ˆæ­¢æ¡ä»¶

- **æˆåŠŸ**: åˆ°è¾¾ç›®æ ‡ç‚¹
- **å¤±è´¥**: ç¢°æ’éšœç¢ç‰©æˆ–åœ°é¢
- **è¶…æ—¶**: è¾¾åˆ°æœ€å¤§æ­¥æ•° (2200æ­¥)

---

## 3. PPOç®—æ³•æ¶æ„

### 3.1 ç‰¹å¾æå–å™¨ (Feature Extractor)

#### **LiDARç‰¹å¾æå– (CNN)**
```python
Input: (36, 4) LiDARæ•°æ®
  â†“
Conv2d(4 channels, kernel=5Ã—3) + ELU
  â†“
Conv2d(16 channels, kernel=5Ã—3, stride=2Ã—1) + ELU  # é™é‡‡æ ·
  â†“
Conv2d(16 channels, kernel=5Ã—3, stride=2Ã—2) + ELU  # é™é‡‡æ ·
  â†“
Flatten â†’ Linear(128) + LayerNorm
  â†“
Output: 128ç»´ç‰¹å¾å‘é‡
```

#### **åŠ¨æ€éšœç¢ç‰©ç‰¹å¾æå– (MLP)**
```python
Input: (5, N) åŠ¨æ€éšœç¢ç‰©ä¿¡æ¯
  â†“
Flatten â†’ MLP[128, 64]
  â†“
Output: 64ç»´ç‰¹å¾å‘é‡
```

#### **ç‰¹å¾èåˆ**
```python
Concatenate:
  - CNNç‰¹å¾ (128ç»´)
  - æ— äººæœºçŠ¶æ€ (stateç»´åº¦)
  - åŠ¨æ€éšœç¢ç‰©ç‰¹å¾ (64ç»´)
  â†“
Total: ~192-256ç»´
  â†“
MLP[256, 256]
  â†“
Output: 256ç»´èåˆç‰¹å¾
```

### 3.2 Actorç½‘ç»œ (ç­–ç•¥ç½‘ç»œ)

- **è¾“å…¥**: 256ç»´ç‰¹å¾å‘é‡
- **è¾“å‡º**: Betaåˆ†å¸ƒå‚æ•° (Î±, Î²)
- **åŠ¨ä½œåˆ†å¸ƒ**: Independent Beta Distribution
  - ä¼˜åŠ¿: è¾“å‡ºè‡ªç„¶é™åˆ¶åœ¨[0,1]åŒºé—´
  - æ˜ å°„åˆ°åŠ¨ä½œç©ºé—´: `action = 2 * normalized_action * limit - limit`
- **åæ ‡è½¬æ¢**: 
  - ç­–ç•¥è¾“å‡º: æœºä½“åæ ‡ç³»é€Ÿåº¦
  - ç¯å¢ƒæ‰§è¡Œ: ä¸–ç•Œåæ ‡ç³»é€Ÿåº¦ (é€šè¿‡`vec_to_world`è½¬æ¢)

### 3.3 Criticç½‘ç»œ (ä»·å€¼ç½‘ç»œ)

- **è¾“å…¥**: 256ç»´ç‰¹å¾å‘é‡
- **è¾“å‡º**: çŠ¶æ€ä»·å€¼ V(s)
- **å½’ä¸€åŒ–**: ä½¿ç”¨ValueNormè¿›è¡Œè¿”å›å€¼å½’ä¸€åŒ–
  - åŸºäºè¿è¡Œå‡å€¼å’Œæ–¹å·®
  - ç¨³å®šè®­ç»ƒè¿‡ç¨‹

### 3.4 PPOè®­ç»ƒæµç¨‹

#### **é˜¶æ®µ1: æ•°æ®æ”¶é›†**
```python
for each training iteration:
    # æ”¶é›† num_envs Ã— training_frame_num æ­¥æ•°æ®
    rollout = collect_data(
        num_envs=1024,        # å¹¶è¡Œç¯å¢ƒæ•°
        frames_per_batch=1024 Ã— 32 = 32768
    )
```

#### **é˜¶æ®µ2: ä¼˜åŠ¿ä¼°è®¡ (GAE)**
```python
# Generalized Advantage Estimation
GAEå‚æ•°:
  - Î³ (gamma) = 0.99      # æŠ˜æ‰£å› å­
  - Î» (lambda) = 0.95     # GAEå¹³æ»‘å‚æ•°

è®¡ç®—è¿‡ç¨‹:
  1. TD-error: Î´_t = r_t + Î³Â·V(s_{t+1}) - V(s_t)
  2. GAE: A_t = Î£(Î³Î»)^i Â· Î´_{t+i}
  3. Return: G_t = A_t + V(s_t)
```

#### **é˜¶æ®µ3: ç­–ç•¥æ›´æ–° (PPO)**

**è¶…å‚æ•°**:
- Training Epochs: 4
- Mini-batches: 16
- Total updates per iteration: 4 Ã— 16 = 64

**æŸå¤±å‡½æ•°**:

1. **Actor Loss (Clipped Surrogate Objective)**
```python
ratio = exp(log Ï€_new(a|s) - log Ï€_old(a|s))
L_actor = -min(
    ratio Â· A,
    clip(ratio, 1-Îµ, 1+Îµ) Â· A
)
å…¶ä¸­ Îµ = 0.1 (clip_ratio)
```

2. **Critic Loss (Clipped Value Loss)**
```python
V_clipped = V_old + clip(V_new - V_old, -Îµ, +Îµ)
L_critic = max(
    HuberLoss(G, V_new),
    HuberLoss(G, V_clipped)
)
ä½¿ç”¨ HuberLoss (delta=10) ç»“åˆL1å’ŒL2æŸå¤±
```

3. **Entropy Loss (æ¢ç´¢é¼“åŠ±)**
```python
L_entropy = -0.001 Ã— mean(H(Ï€))
é¼“åŠ±ç­–ç•¥ä¿æŒä¸€å®šçš„éšæœºæ€§
```

4. **æ€»æŸå¤±**
```python
L_total = L_actor + L_critic + L_entropy
```

#### **é˜¶æ®µ4: æ¢¯åº¦æ›´æ–°**
```python
Optimizers:
  - Feature Extractor: Adam(lr=5e-4)
  - Actor: Adam(lr=5e-4)
  - Critic: Adam(lr=5e-4)

Gradient Clipping:
  - Actor: max_norm = 5.0
  - Critic: max_norm = 5.0
```

---

## 4. è®­ç»ƒé…ç½®

### 4.1 é»˜è®¤è®­ç»ƒå‚æ•°

```yaml
è®­ç»ƒæ€»å¸§æ•°: 1.2e9 (12äº¿å¸§)
è¯„ä¼°é—´éš”: æ¯1000ä¸ªè®­ç»ƒæ­¥
ä¿å­˜é—´éš”: æ¯1000ä¸ªè®­ç»ƒæ­¥

ç¯å¢ƒé…ç½®:
  - å¹¶è¡Œç¯å¢ƒæ•°: 1024
  - æœ€å¤§episodeé•¿åº¦: 2200æ­¥
  - ç¯å¢ƒé—´è·: 8.0ç±³
  - é™æ€éšœç¢ç‰©æ•°: 350
  - åŠ¨æ€éšœç¢ç‰©æ•°: 80
  - åŠ¨æ€éšœç¢ç‰©é€Ÿåº¦: [0.5, 1.5] m/s

PPOé…ç½®:
  - æ¯æ¬¡è¿­ä»£æ”¶é›†å¸§æ•°: 1024 Ã— 32 = 32768
  - è®­ç»ƒepochæ•°: 4
  - Mini-batchæ•°: 16
  - Actorå­¦ä¹ ç‡: 5e-4
  - Criticå­¦ä¹ ç‡: 5e-4
  - Clip ratio: 0.1
  - Entropy coefficient: 1e-3
```

### 4.2 è®­ç»ƒå‘½ä»¤è§£æ

```bash
python training/scripts/train.py \
    headless=True \              # æ— å¤´æ¨¡å¼(æ— æ¸²æŸ“)
    env.num_envs=1024 \          # 1024ä¸ªå¹¶è¡Œç¯å¢ƒ
    env.num_obstacles=350 \      # 350ä¸ªé™æ€éšœç¢ç‰©
    env_dyn.num_obstacles=80 \   # 80ä¸ªåŠ¨æ€éšœç¢ç‰©
    wandb.mode=online            # åœ¨çº¿ç›‘æ§
```

**è®­ç»ƒèµ„æºéœ€æ±‚**:
- GPU: å»ºè®®RTX 3090æˆ–æ›´é«˜ (24GB+ VRAM)
- å†…å­˜: 32GB+ RAM
- å­˜å‚¨: ~10GB (æ¨¡å‹æ£€æŸ¥ç‚¹)

---

## 5. æ§åˆ¶æµç¨‹

### 5.1 å®Œæ•´çš„ä¸€æ­¥æ‰§è¡Œæµç¨‹

```
1. ç­–ç•¥ç½‘ç»œè¾“å‡º (Policy Network)
   â†“
   Betaåˆ†å¸ƒé‡‡æ · â†’ å½’ä¸€åŒ–åŠ¨ä½œ [0,1]
   â†“
   æ˜ å°„åˆ°é€Ÿåº¦èŒƒå›´ [-2, 2] m/s (æœºä½“åæ ‡ç³»)
   â†“

2. åæ ‡è½¬æ¢ (Coordinate Transform)
   â†“
   vec_to_world() â†’ ä¸–ç•Œåæ ‡ç³»é€Ÿåº¦
   â†“

3. ä½å±‚æ§åˆ¶å™¨ (Lee Position Controller)
   â†“
   é€Ÿåº¦æŒ‡ä»¤ â†’ å§¿æ€æ§åˆ¶ â†’ ç”µæœºæ¨åŠ›
   â†“

4. ç‰©ç†ä»¿çœŸ (Isaac Sim)
   â†“
   æ›´æ–°æ— äººæœºçŠ¶æ€
   â†“

5. ä¼ æ„Ÿå™¨æ›´æ–°
   â†“
   LiDARæ‰«æ + çŠ¶æ€è§‚æµ‹
   â†“

6. å¥–åŠ±è®¡ç®—ä¸ç»ˆæ­¢åˆ¤æ–­
   â†“
   è¿”å› (observation, reward, done)
```

### 5.2 è®­ç»ƒå¾ªç¯ä¼ªä»£ç 

```python
for iteration in range(total_iterations):
    # 1. æ•°æ®æ”¶é›†é˜¶æ®µ
    rollout_data = []
    for step in range(frames_per_batch):
        action = policy.select_action(obs)
        next_obs, reward, done = env.step(action)
        rollout_data.append((obs, action, reward, done, value))
    
    # 2. GAEè®¡ç®—
    advantages, returns = compute_gae(rollout_data)
    
    # 3. PPOæ›´æ–°
    for epoch in range(4):
        for minibatch in split_into_minibatches(rollout_data, 16):
            # è®¡ç®—ä¸‰ç§æŸå¤±
            actor_loss = compute_actor_loss(minibatch)
            critic_loss = compute_critic_loss(minibatch)
            entropy_loss = compute_entropy_loss(minibatch)
            
            # æ¢¯åº¦æ›´æ–°
            total_loss = actor_loss + critic_loss + entropy_loss
            total_loss.backward()
            optimizer.step()
    
    # 4. è¯„ä¼°ä¸ä¿å­˜
    if iteration % eval_interval == 0:
        evaluate_policy()
    if iteration % save_interval == 0:
        save_checkpoint()
```

---

## 6. å…³é”®æŠ€æœ¯ç‰¹ç‚¹

### 6.1 å¤šæ¨¡æ€æ„ŸçŸ¥èåˆ
- **è§†è§‰**: CNNæå–LiDARç©ºé—´ç‰¹å¾
- **çŠ¶æ€**: MLPå¤„ç†æ— äººæœºè¿åŠ¨å­¦çŠ¶æ€
- **åŠ¨æ€æ„ŸçŸ¥**: ä¸“é—¨ç½‘ç»œå¤„ç†åŠ¨æ€éšœç¢ç‰©ä¿¡æ¯
- **ç‰¹å¾èåˆ**: å¤šå±‚MLPèåˆå¼‚æ„ä¿¡æ¯

### 6.2 åæ ‡ç³»ç®¡ç†
- **è§‚æµ‹ç©ºé—´**: æœºä½“åæ ‡ç³» (ä¾¿äºå­¦ä¹ )
- **åŠ¨ä½œè¾“å‡º**: æœºä½“åæ ‡ç³»é€Ÿåº¦
- **æ‰§è¡Œç©ºé—´**: ä¸–ç•Œåæ ‡ç³» (ä¾¿äºæ§åˆ¶)
- **è‡ªåŠ¨è½¬æ¢**: `vec_to_world()` å’Œ `vec_to_new_frame()`

### 6.3 è®­ç»ƒç¨³å®šæ€§æŠ€æœ¯
1. **Value Normalization**: å½’ä¸€åŒ–è¿”å›å€¼
2. **Gradient Clipping**: é™åˆ¶æ¢¯åº¦èŒƒæ•°â‰¤5.0
3. **PPO Clipping**: é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦
4. **Huber Loss**: ç»“åˆL1å’ŒL2çš„é²æ£’æŸå¤±
5. **Layer Normalization**: ç¨³å®šç‰¹å¾æå–

### 6.4 å¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒ
- 1024ä¸ªå¹¶è¡Œç¯å¢ƒ
- GPUåŠ é€Ÿç‰©ç†ä»¿çœŸ
- æ‰¹é‡æ•°æ®æ”¶é›†ä¸å¤„ç†
- é«˜æ•ˆçš„ç»éªŒåˆ©ç”¨ (4 epochs Ã— 16 minibatches)

---

## 7. æ€§èƒ½æŒ‡æ ‡

è®­ç»ƒè¿‡ç¨‹ç›‘æ§çš„å…³é”®æŒ‡æ ‡ï¼š

```yaml
ç¯å¢ƒæŒ‡æ ‡:
  - env_frames: æ€»è®­ç»ƒå¸§æ•°
  - rollout_fps: æ•°æ®æ”¶é›†é€Ÿåº¦ (å¸§/ç§’)
  - train/episode_reward: å¹³å‡episodeå¥–åŠ±
  - train/success_rate: è®­ç»ƒæˆåŠŸç‡

è®­ç»ƒæŒ‡æ ‡:
  - actor_loss: Actorç½‘ç»œæŸå¤±
  - critic_loss: Criticç½‘ç»œæŸå¤±
  - entropy: ç­–ç•¥ç†µ (æ¢ç´¢ç¨‹åº¦)
  - actor_grad_norm: Actoræ¢¯åº¦èŒƒæ•°
  - critic_grad_norm: Criticæ¢¯åº¦èŒƒæ•°
  - explained_var: ä»·å€¼å‡½æ•°è§£é‡Šæ–¹å·®

è¯„ä¼°æŒ‡æ ‡:
  - eval/episode_reward: è¯„ä¼°å¹³å‡å¥–åŠ±
  - eval/success_rate: è¯„ä¼°æˆåŠŸç‡
  - eval/avg_steps: å¹³å‡å®Œæˆæ­¥æ•°
```

---

## 8. æ–‡ä»¶ç»“æ„è¯´æ˜

```
isaac-training/
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ train.py          # ä¸»è®­ç»ƒè„šæœ¬
â”‚   â”‚   â”œâ”€â”€ ppo.py            # PPOç®—æ³•å®ç°
â”‚   â”‚   â”œâ”€â”€ env.py            # å¯¼èˆªç¯å¢ƒå®šä¹‰
â”‚   â”‚   â””â”€â”€ utils.py          # å·¥å…·å‡½æ•°
â”‚   â”‚
â”‚   â””â”€â”€ cfg/
â”‚       â”œâ”€â”€ train.yaml        # è®­ç»ƒæ€»é…ç½®
â”‚       â”œâ”€â”€ ppo.yaml          # PPOè¶…å‚æ•°
â”‚       â”œâ”€â”€ drone.yaml        # æ— äººæœºä¸ä¼ æ„Ÿå™¨é…ç½®
â”‚       â””â”€â”€ sim.yaml          # ä»¿çœŸé…ç½®
â”‚
â”œâ”€â”€ third_party/
â”‚   â”œâ”€â”€ OmniDrones/           # æ— äººæœºä»¿çœŸåº“
â”‚   â”œâ”€â”€ orbit/                # Isaac Orbitæ¡†æ¶
â”‚   â””â”€â”€ rl/                   # TorchRLå¼ºåŒ–å­¦ä¹ åº“
â”‚
â””â”€â”€ outputs/                  # è®­ç»ƒè¾“å‡º
    â””â”€â”€ YYYY-MM-DD/
        â””â”€â”€ HH-MM-SS/
            â”œâ”€â”€ checkpoint_*.pt    # æ¨¡å‹æ£€æŸ¥ç‚¹
            â””â”€â”€ logs/              # è®­ç»ƒæ—¥å¿—
```

---

## 9. ä½¿ç”¨ç¤ºä¾‹

### 9.1 å¼€å§‹è®­ç»ƒ

```bash
# åŸºç¡€è®­ç»ƒ (å°è§„æ¨¡æµ‹è¯•)
python training/scripts/train.py

# å¤§è§„æ¨¡è®­ç»ƒ (ç”Ÿäº§ç¯å¢ƒ)
python training/scripts/train.py \
    headless=True \
    env.num_envs=1024 \
    env.num_obstacles=350 \
    env_dyn.num_obstacles=80 \
    wandb.mode=online

# ç»§ç»­è®­ç»ƒ (ä»æ£€æŸ¥ç‚¹æ¢å¤)
python training/scripts/train.py \
    wandb.run_id=<your_run_id>
```

### 9.2 è¯„ä¼°æ¨¡å‹

```bash
# è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹
python training/scripts/eval.py \
    checkpoint_path=outputs/YYYY-MM-DD/HH-MM-SS/checkpoint_final.pt \
    headless=False  # å¯è§†åŒ–
```

---

## 10. æ€»ç»“

NavRLé‡‡ç”¨**PPOç®—æ³•**è®­ç»ƒæ— äººæœºåœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­å¯¼èˆªï¼Œæ ¸å¿ƒç‰¹ç‚¹åŒ…æ‹¬ï¼š

âœ… **å¤šæ¨¡æ€æ„ŸçŸ¥**: LiDAR + çŠ¶æ€ + åŠ¨æ€éšœç¢ç‰©ä¿¡æ¯  
âœ… **åˆ†å±‚ç‰¹å¾æå–**: CNN + MLP èåˆæ¶æ„  
âœ… **Betaåˆ†å¸ƒç­–ç•¥**: è‡ªç„¶çš„åŠ¨ä½œè¾¹ç•Œçº¦æŸ  
âœ… **GAEä¼˜åŠ¿ä¼°è®¡**: å‡å°æ–¹å·®ï¼ŒåŠ é€Ÿæ”¶æ•›  
âœ… **PPOç¨³å®šè®­ç»ƒ**: Clipæœºåˆ¶ä¿è¯ç­–ç•¥å¹³æ»‘æ›´æ–°  
âœ… **å¤§è§„æ¨¡å¹¶è¡Œ**: 1024ç¯å¢ƒ Ã— 32æ­¥ = 32Kæ ·æœ¬/æ¬¡  
âœ… **åæ ‡ç³»ç®¡ç†**: æœºä½“åæ ‡å­¦ä¹  + ä¸–ç•Œåæ ‡æ‰§è¡Œ  

è¯¥æ¶æ„åœ¨Isaac Simä¸­å®ç°äº†é«˜æ•ˆã€ç¨³å®šçš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹ã€‚

---

## å‚è€ƒæ–‡çŒ®

1. Schulman, J., et al. (2017). "Proximal Policy Optimization Algorithms"
2. Schulman, J., et al. (2015). "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
3. NVIDIA Isaac Sim Documentation
4. TorchRL Documentation
5. OmniDrones Framework

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2025-10-21  
**ä½œè€…**: GitHub Copilot
