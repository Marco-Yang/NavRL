# å¯¹æ¯ä¸ªç¯å¢ƒ32æ­¥æ•°æ®è¿›è¡ŒEpisodeå‹ç¼©çš„å¯è¡Œæ€§åˆ†æ

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**ç”Ÿæˆæ—¥æœŸ**: 2025-10-23  
**é—®é¢˜**: èƒ½å¦å¯¹NavRLä¸­æ¯ä¸ªç¯å¢ƒçš„32æ­¥æ•°æ®é‡‡ç”¨ç±»ä¼¼episodeå‹ç¼©çš„æ–¹æ³•è¿›è¡Œå‹ç¼©è§£å‹ï¼Ÿ

---

## 1. å½“å‰æ•°æ®æµåˆ†æ

### 1.1 æ•°æ®æ”¶é›†ç»“æ„

```python
# è®­ç»ƒé…ç½®
num_envs = 1024                    # 1024ä¸ªå¹¶è¡Œç¯å¢ƒ
training_frame_num = 32            # æ¯ä¸ªç¯å¢ƒæ”¶é›†32æ­¥
frames_per_batch = 1024 Ã— 32 = 32768  # æ¯æ‰¹æ€»æ­¥æ•°

# æ•°æ®å½¢çŠ¶
collector = SyncDataCollector(
    frames_per_batch=32768,        # 32K transitions
    return_same_td=True,           # é›¶æ‹·è´ä¼˜åŒ–
)

# æ¯æ¬¡æ”¶é›†çš„æ•°æ®
for data in collector:
    # data æ˜¯ä¸€ä¸ª TensorDict
    # å½¢çŠ¶: [1024, 32, ...] æˆ– reshapeå [32768, ...]
    pass
```

---

### 1.2 è§‚æµ‹æ•°æ®è¯¦ç»†åˆ†æ

#### **æ¯ä¸ªç¯å¢ƒæ¯æ­¥çš„è§‚æµ‹æ•°æ®**

```python
observation = {
    "state": torch.tensor([...]),           # [8] - float32
    "lidar": torch.tensor([...]),           # [1, 36, 4] - float32
    "direction": torch.tensor([...]),       # [2] - float32
    "dynamic_obstacle": torch.tensor([...]) # [5, 10] - float32
}
```

#### **æ•°æ®å¤§å°è®¡ç®—ï¼ˆå•ä¸ªç¯å¢ƒï¼Œå•æ­¥ï¼‰**

| æ•°æ®é¡¹ | å½¢çŠ¶ | å…ƒç´ æ•° | å­—èŠ‚æ•° (float32) |
|--------|------|--------|-----------------|
| `state` | [8] | 8 | 32 B |
| `lidar` | [1, 36, 4] | 144 | 576 B |
| `direction` | [2] | 2 | 8 B |
| `dynamic_obstacle` | [5, 10] | 50 | 200 B |
| **æ€»è®¡** | - | **204** | **816 B** |

#### **æ‰¹æ¬¡æ•°æ®å¤§å°è®¡ç®—**

```
å•ä¸ªç¯å¢ƒ32æ­¥:
  816 B/step Ã— 32 steps = 26.1 KB

1024ä¸ªç¯å¢ƒ32æ­¥:
  26.1 KB Ã— 1024 = 26.7 MB

åŠ ä¸Šaction, reward, doneç­‰:
  æ€»è®¡çº¦ 35-40 MB/batch
```

---

## 2. Episodeå‹ç¼©æŠ€æœ¯å›é¡¾

### 2.1 replay_buffer.pyçš„å‹ç¼©æ–¹æ³•

```python
def compress_image_seq(images, level=9):
    """
    æ‰¹é‡å‹ç¼©å›¾åƒåºåˆ—
    Input: [batch, C, H, W]
    """
    # 1. æ‹¼æ¥æ‰€æœ‰å›¾åƒ
    concatenated = np.concatenate(images, axis=0).tobytes()
    
    # 2. LZ4å‹ç¼©
    compressed = lz4.frame.compress(concatenated, compression_level=level)
    
    return compressed

# è§£å‹
def decompress_image_seq(compressed_data, image_shape, batch_size, dtype=np.uint8):
    decompressed = lz4.frame.decompress(compressed_data)
    flat_array = np.frombuffer(decompressed, dtype=dtype)
    return flat_array.reshape(batch_size, *image_shape)
```

**å…³é”®ç‰¹ç‚¹**:
- **æ‰¹é‡å‹ç¼©**: æ•´ä¸ªåºåˆ—ä¸€èµ·å‹ç¼©ï¼Œåˆ©ç”¨æ—¶é—´è¿ç»­æ€§
- **æ›´é«˜å‹ç¼©ç‡**: ç›¸é‚»å¸§ç›¸ä¼¼åº¦é«˜ï¼Œå‹ç¼©æ•ˆæœæ›´å¥½
- **è§£å‹å¼€é”€**: éœ€è¦è§£å‹æ•´ä¸ªåºåˆ—æ‰èƒ½è®¿é—®å•å¸§

---

## 3. åœ¨NavRLä¸­åº”ç”¨çš„å¯è¡Œæ€§åˆ†æ

### 3.1 ä¼˜åŠ¿åˆ†æ âœ…

#### **ä¼˜åŠ¿1: æ—¶é—´è¿ç»­æ€§å¼º**

```python
# ç›¸é‚»æ­¥çš„LiDARæ•°æ®ç›¸ä¼¼åº¦å¾ˆé«˜
lidar_t0 = [4.2, 4.1, 4.3, ...]  # ç¬¬0æ­¥
lidar_t1 = [4.2, 4.1, 4.2, ...]  # ç¬¬1æ­¥ï¼ˆå‡ ä¹ç›¸åŒï¼‰
lidar_t2 = [4.1, 4.0, 4.2, ...]  # ç¬¬2æ­¥ï¼ˆå¾®å°å˜åŒ–ï¼‰

# LZ4èƒ½æœ‰æ•ˆå‹ç¼©è¿™ç§å†—ä½™
```

**é¢„æœŸå‹ç¼©ç‡**: 
- **LiDARæ•°æ®**: 3-5x (æ—¶é—´è¿ç»­æ€§é«˜)
- **Stateæ•°æ®**: 2-3x (å˜åŒ–è¾ƒæ…¢)
- **æ€»ä½“**: 2.5-4x

---

#### **ä¼˜åŠ¿2: æ•°æ®é‡é€‚ä¸­**

```python
# æ¯ä¸ªç¯å¢ƒ32æ­¥ = 26.1 KB
# å‹ç¼©åé¢„æœŸ: 6-10 KB
# 1024ä¸ªç¯å¢ƒ: 6-10 MB (å‹ç¼©å)
```

**å†…å­˜å ç”¨å¯æ§**: 
- å³ä½¿å…¨éƒ¨å‹ç¼©ï¼Œå†…å­˜å ç”¨ä»åœ¨åˆç†èŒƒå›´å†…
- ä¸ä¼šåƒå¤§è§„æ¨¡replay bufferé‚£æ ·æœ‰å†…å­˜å‹åŠ›

---

#### **ä¼˜åŠ¿3: ä¸PPOè®­ç»ƒæµç¨‹å…¼å®¹**

```python
# PPOè®­ç»ƒæµç¨‹
for data in collector:
    # 1. æ”¶é›†æ•°æ® (å¯ä»¥å‹ç¼©å­˜å‚¨)
    compressed_data = compress_rollout(data)
    
    # 2. è®­ç»ƒæ—¶è§£å‹
    for epoch in range(4):  # PPOè®­ç»ƒ4ä¸ªepoch
        for minibatch in make_batches(data, 16):
            # åªè§£å‹éœ€è¦çš„minibatch
            decompressed = decompress_minibatch(compressed_data, minibatch_idx)
            loss = policy.train(decompressed)
```

**å…³é”®ç‚¹**: PPOéœ€è¦å¤šepochè®­ç»ƒåŒä¸€æ‰¹æ•°æ®ï¼Œå¯ä»¥:
1. æ”¶é›†æ—¶å‹ç¼©å­˜å‚¨
2. æ¯ä¸ªepochè§£å‹ä½¿ç”¨
3. 4ä¸ªepochåä¸¢å¼ƒ

---

### 3.2 æŒ‘æˆ˜åˆ†æ âš ï¸

#### **æŒ‘æˆ˜1: å½“å‰æ˜¯é›¶æ‹·è´æ¨¡å¼**

```python
collector = SyncDataCollector(
    return_same_td=True,  # âš ï¸ é›¶æ‹·è´ä¼˜åŒ–
)

# å¦‚æœè¦å‹ç¼©ï¼Œå¿…é¡»æ”¹ä¸º
collector = SyncDataCollector(
    return_same_td=False,  # éœ€è¦å…‹éš†æ•°æ®
)
```

**å½±å“**:
- å†…å­˜å ç”¨ç¿»å€: 2 GB â†’ 4 GB
- éœ€è¦æ•°æ®æ‹·è´å¼€é”€

---

#### **æŒ‘æˆ˜2: å‹ç¼©/è§£å‹å¼€é”€**

**å‹ç¼©æ—¶é—´ä¼°ç®—**:
```python
# LZ4å‹ç¼©é€Ÿåº¦: ~500 MB/s (å•çº¿ç¨‹)
# æ•°æ®é‡: 35 MB/batch
# å‹ç¼©æ—¶é—´: 35 MB / 500 MB/s = 70 ms
```

**è§£å‹æ—¶é—´ä¼°ç®—**:
```python
# LZ4è§£å‹é€Ÿåº¦: ~2 GB/s
# å‹ç¼©åæ•°æ®: ~10 MB
# è§£å‹æ—¶é—´: 10 MB / 2000 MB/s = 5 ms
```

**è®­ç»ƒæ—¶é—´å¯¹æ¯”**:
```python
# å½“å‰PPOè®­ç»ƒæ—¶é—´: ~50-100 ms/batch
# å‹ç¼©å¼€é”€: 70 ms (ä¸€æ¬¡æ€§)
# è§£å‹å¼€é”€: 5 ms Ã— 4 epochs Ã— 16 minibatches = 320 ms

# æ€»å¼€é”€: 70 + 320 = 390 ms
# vs è®­ç»ƒæ—¶é—´: 50-100 ms

# âš ï¸ å¼€é”€å¯èƒ½è¶…è¿‡è®­ç»ƒæ—¶é—´ï¼
```

---

#### **æŒ‘æˆ˜3: GPU-CPUä¼ è¾“å¼€é”€**

```python
# å½“å‰æµç¨‹ (å…¨GPU)
GPU: Collect data â†’ Train â†’ Discard
     â†‘_____________â†“

# å‹ç¼©æµç¨‹ (éœ€è¦CPU)
GPU: Collect data â†’ Copy to CPU â†’ Compress â†’ Store
                                    â†“
CPU: Decompress â†’ Copy to GPU â†’ Train
     â†‘_______________â†“

# é¢å¤–å¼€é”€
# GPUâ†’CPU: ~10 GB/s â†’ 35 MB / 10 GB/s = 3.5 ms
# CPUâ†’GPU: ~10 GB/s â†’ 10 MB / 10 GB/s = 1 ms
# æ€»è®¡: ~5 ms Ã— å¤šæ¬¡ = æ˜¾è‘—å¼€é”€
```

---

#### **æŒ‘æˆ˜4: Isaac Simçš„GPUä¼˜åŒ–**

NavRLå·²ç»é«˜åº¦ä¼˜åŒ–:
- **1024ä¸ªç¯å¢ƒå¹¶è¡Œ**åœ¨GPUä¸Š
- **é›¶æ‹·è´TensorDict**ä¼ é€’
- **å…¨æµç¨‹GPUåŠ é€Ÿ** (æ— CPUç“¶é¢ˆ)

**å¼•å…¥å‹ç¼©å¯èƒ½ç ´åç°æœ‰ä¼˜åŒ–**:
- éœ€è¦GPUâ†’CPUä¼ è¾“
- éœ€è¦æ•°æ®å…‹éš† (`return_same_td=False`)
- å¯èƒ½é™ä½æ•´ä½“ååé‡

---

## 4. é€‚ç”¨åœºæ™¯åˆ¤æ–­

### 4.1 âœ… **é€‚åˆå‹ç¼©çš„åœºæ™¯**

#### **åœºæ™¯1: å†…å­˜å—é™ç¯å¢ƒ**

```python
# å¦‚æœå†…å­˜ä¸è¶³ä»¥å­˜å‚¨å®Œæ•´batch
# ä¾‹å¦‚: åªæœ‰4 GB GPUå†…å­˜

# å‹ç¼©å¯ä»¥èŠ‚çœå†…å­˜
æœªå‹ç¼©: 35 MB/batch Ã— 3 batches = 105 MB
å‹ç¼©å: 10 MB/batch Ã— 3 batches = 30 MB
èŠ‚çœ: 75 MB
```

**åˆ¤æ–­æ¡ä»¶**: GPUå†…å­˜ < 8 GB

---

#### **åœºæ™¯2: éœ€è¦ä¿å­˜rolloutå†å²**

```python
# å¦‚æœéœ€è¦ä¿å­˜å¤šæ‰¹æ•°æ®ç”¨äºè°ƒè¯•æˆ–åˆ†æ
rollout_history = []

for i, data in enumerate(collector):
    # å‹ç¼©å­˜å‚¨å†å²
    compressed = compress_rollout(data)
    rollout_history.append(compressed)
    
    # è®­ç»ƒæ—¶è§£å‹
    decompressed = decompress_rollout(compressed)
    policy.train(decompressed)

# èŠ‚çœç£ç›˜/å†…å­˜
æœªå‹ç¼©: 35 MB Ã— 1000 rollouts = 35 GB
å‹ç¼©å: 10 MB Ã— 1000 rollouts = 10 GB
```

**åˆ¤æ–­æ¡ä»¶**: éœ€è¦é•¿æœŸå­˜å‚¨rolloutæ•°æ®

---

#### **åœºæ™¯3: åˆ†å¸ƒå¼è®­ç»ƒï¼ˆè·¨æœºå™¨ä¼ è¾“ï¼‰**

```python
# å¦‚æœéœ€è¦åœ¨å¤šå°æœºå™¨é—´ä¼ è¾“æ•°æ®
# Machine A: æ”¶é›†æ•°æ®
compressed_data = compress_rollout(data)

# ç½‘ç»œä¼ è¾“ (å‹ç¼©åæ›´å¿«)
send_to_machine_B(compressed_data)  # 10 MB vs 35 MB

# Machine B: è®­ç»ƒ
data = decompress_rollout(compressed_data)
policy.train(data)
```

**åˆ¤æ–­æ¡ä»¶**: ç½‘ç»œå¸¦å®½ < 1 Gb/s

---

### 4.2 âŒ **ä¸é€‚åˆå‹ç¼©çš„åœºæ™¯**

#### **åœºæ™¯1: å½“å‰NavRLé…ç½® (é»˜è®¤)**

```python
# é…ç½®
num_envs = 1024
frames_per_batch = 32768
GPUå†…å­˜: 24 GB (å……è¶³)
return_same_td = True (é›¶æ‹·è´)

# ç“¶é¢ˆåˆ†æ
CPU-GPUä¼ è¾“: ä¸æ˜¯ç“¶é¢ˆ (å…¨GPUæµç¨‹)
å†…å­˜: ä¸æ˜¯ç“¶é¢ˆ (2-4 GB << 24 GB)
ç£ç›˜: ä¸æ˜¯ç“¶é¢ˆ (æ•°æ®ç«‹å³ä¸¢å¼ƒ)

# ç»“è®º: âŒ å‹ç¼©æ— ç›Šï¼Œåè€Œå¢åŠ å¼€é”€
```

---

#### **åœºæ™¯2: è®­ç»ƒé€Ÿåº¦ä¼˜å…ˆ**

```python
# å½“å‰è®­ç»ƒåå: ~10,000 frames/s
# åŠ å‹ç¼©å: ~8,000 frames/s (é™ä½20%)

# åŸå› : å‹ç¼©/è§£å‹å¼€é”€ + GPU-CPUä¼ è¾“
```

---

## 5. æ¨èæ–¹æ¡ˆ

### 5.1 æ–¹æ¡ˆA: ä¸å‹ç¼© (æ¨è) â­

**é€‚ç”¨äº**: å½“å‰NavRLé»˜è®¤é…ç½®

**ç†ç”±**:
1. âœ… GPUå†…å­˜å……è¶³ (24 GB >> 4 GBéœ€æ±‚)
2. âœ… å…¨GPUæµç¨‹å·²é«˜åº¦ä¼˜åŒ–
3. âœ… é›¶æ‹·è´æ¨¡å¼æ€§èƒ½æœ€ä½³
4. âœ… æ•°æ®ç«‹å³ä¸¢å¼ƒï¼Œæ— å­˜å‚¨éœ€æ±‚

**ç»“è®º**: **ä¿æŒç°çŠ¶ï¼Œä¸å¼•å…¥å‹ç¼©**

---

### 5.2 æ–¹æ¡ˆB: é€‰æ‹©æ€§å‹ç¼© (ä¸­ç­‰éœ€æ±‚)

**é€‚ç”¨äº**: éœ€è¦ä¿å­˜éƒ¨åˆ†rolloutæ•°æ®ç”¨äºåˆ†æ

**å®ç°**:
```python
# åªå‹ç¼©éœ€è¦ä¿å­˜çš„æ•°æ®
for i, data in enumerate(collector):
    # æ­£å¸¸è®­ç»ƒ (æ— å‹ç¼©)
    policy.train(data)
    
    # å¶å°”ä¿å­˜ (å‹ç¼©)
    if i % 100 == 0:
        compressed = compress_rollout_for_storage(data)
        save_to_disk(compressed, f'rollout_{i}.lz4')
```

**ä¼˜åŠ¿**:
- ä¸å½±å“è®­ç»ƒé€Ÿåº¦
- èŠ‚çœå­˜å‚¨ç©ºé—´
- å¯ç”¨äºäº‹ååˆ†æ

---

### 5.3 æ–¹æ¡ˆC: å®Œå…¨å‹ç¼© (ç‰¹æ®Šåœºæ™¯)

**é€‚ç”¨äº**: å†…å­˜ä¸¥é‡å—é™ (GPU < 8 GB)

**å®ç°**:
```python
class CompressedDataCollector:
    """å‹ç¼©æ•°æ®æ”¶é›†å™¨"""
    def __init__(self, base_collector, compression_level=1):
        self.base_collector = base_collector
        self.compression_level = compression_level
    
    def __iter__(self):
        for data in self.base_collector:
            # 1. æ”¶é›†æ•°æ®
            # 2. å‹ç¼©LiDARæ•°æ® (æœ€å¤§çš„éƒ¨åˆ†)
            compressed_data = self._compress_observations(data)
            
            yield compressed_data
    
    def _compress_observations(self, data):
        """å‹ç¼©è§‚æµ‹æ•°æ®"""
        # æå–LiDARæ•°æ®
        lidar = data["agents", "observation", "lidar"]  # [1024, 32, 1, 36, 4]
        
        # å‹ç¼© (æŒ‰ç¯å¢ƒ)
        compressed_lidar = []
        for env_id in range(self.num_envs):
            env_lidar = lidar[env_id].cpu().numpy()  # [32, 1, 36, 4]
            compressed = lz4.frame.compress(
                env_lidar.tobytes(),
                compression_level=self.compression_level
            )
            compressed_lidar.append(compressed)
        
        # æ›¿æ¢ä¸ºå‹ç¼©ç‰ˆæœ¬
        data["_compressed_lidar"] = compressed_lidar
        del data["agents"]["observation"]["lidar"]
        
        return data
    
    def _decompress_observations(self, data, minibatch_indices):
        """è§£å‹éœ€è¦çš„minibatch"""
        # åªè§£å‹è¢«é‡‡æ ·çš„ç¯å¢ƒ
        compressed_lidar = data["_compressed_lidar"]
        
        decompressed_lidar = []
        for idx in minibatch_indices:
            env_id = idx // 32  # è®¡ç®—ç¯å¢ƒID
            step = idx % 32     # è®¡ç®—æ­¥æ•°
            
            # è§£å‹æ•´ä¸ªç¯å¢ƒ
            if env_id not in self._cache:
                env_lidar_bytes = lz4.frame.decompress(compressed_lidar[env_id])
                env_lidar = np.frombuffer(env_lidar_bytes, dtype=np.float32)
                env_lidar = env_lidar.reshape(32, 1, 36, 4)
                self._cache[env_id] = torch.from_numpy(env_lidar).to(self.device)
            
            # æå–éœ€è¦çš„æ­¥
            decompressed_lidar.append(self._cache[env_id][step])
        
        return torch.stack(decompressed_lidar)

# ä½¿ç”¨
collector = CompressedDataCollector(
    SyncDataCollector(...),
    compression_level=1  # ä½å‹ç¼©çº§åˆ«ï¼Œé€Ÿåº¦ä¼˜å…ˆ
)
```

**ä¼˜ç¼ºç‚¹**:
- âœ… èŠ‚çœå†…å­˜: 35 MB â†’ 10 MB
- âœ… å¯æ§çš„å‹ç¼©çº§åˆ«
- âŒ å¢åŠ CPUå¼€é”€
- âŒ éœ€è¦GPU-CPUä¼ è¾“
- âŒ å¤æ‚åº¦æé«˜

---

## 6. æ€§èƒ½é¢„æµ‹å¯¹æ¯”

### 6.1 æ–¹æ¡ˆå¯¹æ¯”è¡¨

| æ–¹æ¡ˆ | å†…å­˜å ç”¨ | è®­ç»ƒé€Ÿåº¦ | å®ç°å¤æ‚åº¦ | æ¨èåº¦ |
|------|---------|---------|-----------|--------|
| **A: ä¸å‹ç¼©** | 35 MB | 10,000 fps | ç®€å• | â­â­â­â­â­ |
| **B: é€‰æ‹©æ€§å‹ç¼©** | 35 MB (è®­ç»ƒ)<br>10 MB (å­˜å‚¨) | 10,000 fps | ä¸­ç­‰ | â­â­â­â­ |
| **C: å®Œå…¨å‹ç¼©** | 10 MB | 8,000 fps | å¤æ‚ | â­â­ |

---

### 6.2 å†…å­˜èŠ‚çœ vs æ€§èƒ½æŸå¤±

```
æ–¹æ¡ˆC (å®Œå…¨å‹ç¼©):
  å†…å­˜èŠ‚çœ: 25 MB/batch
  æ€§èƒ½æŸå¤±: ~20% è®­ç»ƒé€Ÿåº¦

ROI (æŠ•èµ„å›æŠ¥ç‡):
  èŠ‚çœå†…å­˜: 25 MB
  ä»£ä»·: 20% é€Ÿåº¦ â†’ è®­ç»ƒæ—¶é—´å¢åŠ  25%
  
  é™¤éGPUå†…å­˜ < 8 GBï¼Œå¦åˆ™ä¸å€¼å¾—
```

---

## 7. å®æ–½å»ºè®®

### 7.1 å½“å‰NavRL (GPU â‰¥ 16 GB)

**å»ºè®®**: **æ–¹æ¡ˆA - ä¸å‹ç¼©** â­

**ç†ç”±**:
1. å†…å­˜å……è¶³ï¼Œæ— å‹ç¼©å¿…è¦
2. å½“å‰é›¶æ‹·è´ä¼˜åŒ–å·²æ˜¯æœ€ä¼˜
3. å¼•å…¥å‹ç¼©ä¼šé™ä½æ€§èƒ½

**è¡ŒåŠ¨**: ä¿æŒç°çŠ¶

---

### 7.2 å¦‚æœéœ€è¦ä¿å­˜rolloutæ•°æ®

**å»ºè®®**: **æ–¹æ¡ˆB - é€‰æ‹©æ€§å‹ç¼©**

**å®ç°**:
```python
# åœ¨train.pyä¸­æ·»åŠ 
import lz4.frame

def save_rollout_compressed(data, path):
    """ä¿å­˜å‹ç¼©çš„rolloutæ•°æ®"""
    # è½¬æ¢ä¸ºnumpy
    data_np = {
        key: value.cpu().numpy() 
        for key, value in data.items(True, True)
    }
    
    # åºåˆ—åŒ–
    import pickle
    serialized = pickle.dumps(data_np)
    
    # å‹ç¼©
    compressed = lz4.frame.compress(serialized, compression_level=9)
    
    # ä¿å­˜
    with open(path, 'wb') as f:
        f.write(compressed)
    
    print(f"Saved compressed rollout: {len(compressed)/1e6:.2f} MB")

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for i, data in enumerate(collector):
    policy.train(data)
    
    # æ¯100æ¬¡ä¿å­˜ä¸€æ¬¡
    if i % 100 == 0:
        save_rollout_compressed(data, f'rollouts/rollout_{i}.lz4')
```

---

### 7.3 å¦‚æœGPUå†…å­˜ < 8 GB

**å»ºè®®**: **æ–¹æ¡ˆC - å®Œå…¨å‹ç¼©** (ä½†å…ˆå°è¯•å‡å°‘batch size)

**ä¼˜å…ˆå°è¯•**:
```python
# æ–¹æ¡ˆ1: å‡å°‘batch size (æ›´ç®€å•)
frames_per_batch = 1024 * 16  # å‡åŠ

# æ–¹æ¡ˆ2: å‡å°‘ç¯å¢ƒæ•°
num_envs = 512  # å‡åŠ

# æ–¹æ¡ˆ3: å¦‚æœä»ä¸å¤Ÿï¼Œå†è€ƒè™‘å‹ç¼©
```

---

## 8. ä»£ç å®ç°ç¤ºä¾‹ (æ–¹æ¡ˆB)

### 8.1 é€‰æ‹©æ€§å‹ç¼©å·¥å…·

```python
# æ–‡ä»¶: training/scripts/rollout_storage.py
import lz4.frame
import pickle
import torch
from pathlib import Path

class RolloutStorage:
    """Rolloutæ•°æ®å­˜å‚¨å·¥å…· (å¸¦å‹ç¼©)"""
    def __init__(self, save_dir, compression_level=9):
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.compression_level = compression_level
    
    def save(self, data, iteration):
        """ä¿å­˜å‹ç¼©çš„rollout"""
        # è½¬æ¢ä¸ºnumpy (CPU)
        data_dict = {}
        for key in data.keys(True, True):
            if isinstance(key, tuple):
                nested_key = '/'.join(key)
            else:
                nested_key = key
            
            value = data[key]
            if isinstance(value, torch.Tensor):
                data_dict[nested_key] = value.cpu().numpy()
            else:
                data_dict[nested_key] = value
        
        # åºåˆ—åŒ–
        serialized = pickle.dumps(data_dict, protocol=4)
        original_size = len(serialized)
        
        # å‹ç¼©
        compressed = lz4.frame.compress(
            serialized,
            compression_level=self.compression_level
        )
        compressed_size = len(compressed)
        
        # ä¿å­˜
        save_path = self.save_dir / f'rollout_{iteration:06d}.lz4'
        with open(save_path, 'wb') as f:
            f.write(compressed)
        
        compression_ratio = original_size / compressed_size
        print(f"[Rollout] Saved: {save_path.name}")
        print(f"          Size: {original_size/1e6:.2f} MB â†’ {compressed_size/1e6:.2f} MB")
        print(f"          Compression: {compression_ratio:.2f}x")
        
        return save_path
    
    def load(self, iteration, device='cuda:0'):
        """åŠ è½½rolloutæ•°æ®"""
        load_path = self.save_dir / f'rollout_{iteration:06d}.lz4'
        
        # è¯»å–å‹ç¼©æ•°æ®
        with open(load_path, 'rb') as f:
            compressed = f.read()
        
        # è§£å‹
        decompressed = lz4.frame.decompress(compressed)
        
        # ååºåˆ—åŒ–
        data_dict = pickle.loads(decompressed)
        
        # è½¬æ¢å›tensor
        from tensordict import TensorDict
        tensor_dict = {}
        for key, value in data_dict.items():
            nested_keys = key.split('/')
            if len(nested_keys) == 1:
                tensor_dict[key] = torch.from_numpy(value).to(device)
            else:
                # å¤„ç†åµŒå¥—é”®
                current = tensor_dict
                for k in nested_keys[:-1]:
                    if k not in current:
                        current[k] = {}
                    current = current[k]
                current[nested_keys[-1]] = torch.from_numpy(value).to(device)
        
        print(f"[Rollout] Loaded: {load_path.name}")
        return TensorDict(tensor_dict, batch_size=[])

# ä½¿ç”¨ç¤ºä¾‹
storage = RolloutStorage(save_dir='./rollout_data', compression_level=9)

for i, data in enumerate(collector):
    # æ­£å¸¸è®­ç»ƒ
    loss = policy.train(data)
    
    # å®šæœŸä¿å­˜
    if i % 100 == 0:
        storage.save(data, iteration=i)
```

---

### 8.2 é›†æˆåˆ°train.py

```python
# åœ¨train.pyä¸­æ·»åŠ 
from rollout_storage import RolloutStorage

# åˆå§‹åŒ–å­˜å‚¨
rollout_storage = RolloutStorage(
    save_dir=os.path.join(run.dir, 'rollouts'),
    compression_level=9
)

# è®­ç»ƒå¾ªç¯
for i, data in enumerate(collector):
    # è®­ç»ƒ
    with perf_monitor.timer('policy_training'):
        train_loss_stats = policy.train(data)
    
    # å®šæœŸä¿å­˜rollout (å‹ç¼©)
    if i % cfg.get('rollout_save_interval', 100) == 0:
        with perf_monitor.timer('rollout_save'):
            rollout_storage.save(data, iteration=i)
```

---

## 9. æ€»ç»“ä¸æœ€ç»ˆå»ºè®®

### 9.1 æ ¸å¿ƒç»“è®º

#### **å¯¹äºå½“å‰NavRLé…ç½®**:
- âœ… **æ¨è**: æ–¹æ¡ˆA - ä¸å‹ç¼©
- âœ… **ç†ç”±**: å†…å­˜å……è¶³ï¼Œæ€§èƒ½ä¼˜å…ˆ
- âŒ **ä¸æ¨è**: æ–¹æ¡ˆC - å®Œå…¨å‹ç¼©
- âŒ **ç†ç”±**: æ€§èƒ½æŸå¤± > å†…å­˜èŠ‚çœ

#### **å¦‚æœæœ‰ç‰¹æ®Šéœ€æ±‚**:
- ğŸ“Š **éœ€è¦ä¿å­˜æ•°æ®**: æ–¹æ¡ˆB - é€‰æ‹©æ€§å‹ç¼©
- ğŸ’¾ **å†…å­˜ä¸¥é‡å—é™**: å…ˆå‡å°batch sizeï¼Œå†è€ƒè™‘æ–¹æ¡ˆC

---

### 9.2 å†³ç­–æ ‘

```
æ˜¯å¦éœ€è¦ä¿å­˜rolloutå†å²æ•°æ®ï¼Ÿ
â”œâ”€ å¦ â†’ æ–¹æ¡ˆA (ä¸å‹ç¼©) â­â­â­â­â­
â””â”€ æ˜¯
   â”œâ”€ åªéœ€å¶å°”ä¿å­˜ â†’ æ–¹æ¡ˆB (é€‰æ‹©æ€§å‹ç¼©) â­â­â­â­
   â””â”€ éœ€è¦ä¿å­˜æ‰€æœ‰ â†’ æ–¹æ¡ˆB (ä½†è€ƒè™‘ç£ç›˜ç©ºé—´)

GPUå†…å­˜æ˜¯å¦ < 8 GBï¼Ÿ
â”œâ”€ å¦ â†’ æ–¹æ¡ˆA (ä¸å‹ç¼©) â­â­â­â­â­
â””â”€ æ˜¯
   â”œâ”€ èƒ½å¦å‡å°batch sizeï¼Ÿ
   â”‚  â”œâ”€ æ˜¯ â†’ å‡å°batch size (æ›´ç®€å•)
   â”‚  â””â”€ å¦ â†’ æ–¹æ¡ˆC (å®Œå…¨å‹ç¼©) â­â­
   â””â”€ è®­ç»ƒé€Ÿåº¦æ˜¯å¦å…³é”®ï¼Ÿ
      â”œâ”€ æ˜¯ â†’ å‡çº§ç¡¬ä»¶
      â””â”€ å¦ â†’ æ–¹æ¡ˆC (å®Œå…¨å‹ç¼©)
```

---

### 9.3 å®æ–½ä¼˜å…ˆçº§

#### **Phase 1: ä¿æŒç°çŠ¶** (ç«‹å³)
- âœ… ä¸å¼•å…¥è®­ç»ƒæ—¶å‹ç¼©
- âœ… ä¿æŒé›¶æ‹·è´ä¼˜åŒ–
- âœ… ç»§ç»­ä½¿ç”¨å·²å®æ–½çš„checkpointå‹ç¼©

#### **Phase 2: å¯é€‰åŠŸèƒ½** (å¦‚éœ€è¦)
- â³ å®ç°æ–¹æ¡ˆB (é€‰æ‹©æ€§ä¿å­˜rollout)
- â³ ç”¨äºè°ƒè¯•å’Œæ•°æ®åˆ†æ
- â³ ä¸å½±å“è®­ç»ƒæ€§èƒ½

#### **Phase 3: æç«¯åœºæ™¯** (ä»…åœ¨å¿…è¦æ—¶)
- ğŸ”¶ å®ç°æ–¹æ¡ˆC (å®Œå…¨å‹ç¼©)
- ğŸ”¶ ä»…ç”¨äºå†…å­˜ä¸¥é‡å—é™çš„ç¡¬ä»¶
- ğŸ”¶ æ¥å—20%æ€§èƒ½æŸå¤±

---

### 9.4 ä¸å·²å®æ–½ä¼˜åŒ–çš„å…³ç³»

| ä¼˜åŒ–é¡¹ | çŠ¶æ€ | é€‚ç”¨é˜¶æ®µ | æ”¶ç›Š |
|-------|------|---------|------|
| **Checkpointå‹ç¼©** | âœ… å·²å®æ–½ | ä¿å­˜é˜¶æ®µ | 65%ç£ç›˜èŠ‚çœ |
| **å¼‚æ­¥ä¿å­˜** | âœ… å·²å®æ–½ | ä¿å­˜é˜¶æ®µ | 99.9%æ—¶é—´èŠ‚çœ |
| **Rolloutå‹ç¼©** | â³ å¯é€‰ | å­˜å‚¨é˜¶æ®µ | 70%ç£ç›˜èŠ‚çœ |
| **è®­ç»ƒæ•°æ®å‹ç¼©** | âŒ ä¸æ¨è | è®­ç»ƒé˜¶æ®µ | è´Ÿæ”¶ç›Š |

**å…³é”®åŒºåˆ«**:
- **Checkpoint**: I/Oå¯†é›†ï¼Œå‹ç¼©æœ‰ç›Š âœ…
- **Training data**: è®¡ç®—å¯†é›†ï¼Œå‹ç¼©æœ‰å®³ âŒ

---

## 10. å‚è€ƒæ–‡æ¡£

- `REPLAY_BUFFER_VS_SYNCDATACOLLECTOR.md`: å‹ç¼©æŠ€æœ¯è¯¦è§£
- `OPTIMIZATION_IMPLEMENTATION.md`: å·²å®æ–½çš„checkpointå‹ç¼©
- `ALGORITHM_ARCHITECTURE.md`: PPOè®­ç»ƒæµç¨‹
- `POINTCLOUD_PPO_TRAINING.md`: æ•°æ®ç»“æ„è¯¦è§£

---

**æœ€ç»ˆå»ºè®®**: å¯¹äºå½“å‰NavRLé¡¹ç›®ï¼Œ**ä¸å»ºè®®å¯¹è®­ç»ƒæ—¶çš„32æ­¥æ•°æ®è¿›è¡Œå‹ç¼©**ã€‚å·²å®æ–½çš„checkpointå‹ç¼©å·²ç»æä¾›äº†è¶³å¤Ÿçš„ä¼˜åŒ–ï¼Œç»§ç»­å‹ç¼©è®­ç»ƒæ•°æ®ä¼šå¸¦æ¥æ€§èƒ½æŸå¤±è€Œæ— å®è´¨æ”¶ç›Šã€‚

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-10-23  
**ç»´æŠ¤è€…**: GitHub Copilot
